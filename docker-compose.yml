services:
  languagetool:
    hostname: languagetool
    build:
      context: ./docker/languagetool
      dockerfile: Dockerfile
    container_name: languagetool
    ports:
      - ${LANGUAGE_TOOL_PORT}:8010 # Using default port from the image
    environment:
      - langtool_languageModel=/languagetool/.cache/ngrams # OPTIONAL: Using ngrams data
      - Java_Xms=512m
      - Java_Xmx=2g
    volumes:
      - ${LANGUAGE_TOOL_CACHE_DIR}:/languagetool/.cache/ngrams

  vllm_qwen25_32b:
    hostname: vllm_qwen25_32b
    image: vllm/vllm-openai:v0.8.4
    container_name: vllm_qwen25_32b
    ports:
      - ${LLM_API_PORT}:8000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ipc: host
    volumes:
      - ${HUGGING_FACE_CACHE_DIR}:/root/.cache/huggingface
    command: >
      --port 8000 --model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 --quantization "gptq_marlin" --kv-cache-dtype "fp8_e5m2" --tool-call-parser "hermes" --max-model-len 16000 --max-num-batched-tokens 4096 --max-num-seqs 16 --enable-prefix-caching --enable-chunked-prefill --gpu-memory-utilization 0.9 --tensor-parallel-size 2
